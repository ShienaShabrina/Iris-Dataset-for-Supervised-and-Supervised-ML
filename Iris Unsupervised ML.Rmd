---
title: "Homework - Supervised ML in R"
author: "Shiena"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Homework - Supervised ML in R

First, make sure you've loaded the following libraries.

```{r}
library(tidyverse)
library(rsample)
#install.packages("class")
library(class)
#install.packages('gmodels')
library(gmodels)
#install.packages('caret')
library(caret)
#install.packages('rpart')
library(rpart)
#install.packages('rpart.plot')
library(rpart.plot)
```

## Instruksi

Pertama read file `iris_homework`

```{r}
data = readRDS('iris_homework')
```

Kemudian, ikuti langkah-langkah berikut:

1.  Pecah data menjadi training dan test with `set.seed(100)`.
2.  Buatlah sebuah algoritma KNN menggunakan `set.seed(1)`. Coba dari nilai k = 1 sampai k = 10. Laporkan nilai k yang memberikan akurasi tertinggi. Berikan juga nilai akurasi tertinggi yang dicapai model.
3.  Buatlah sebuah algoritma logistic regression menggunakan `set.seed(1)`. Kolom mana saja yang memiliki p-value di bawah 0.05? Tuliskan kolom-kolom tersebut. Kemudian, buat ulang regresi logistik hanya dengan kolom-kolom dengan p-value di bawah 0.05, dan laporkan nilai akurasi tertingginya.

### Tempat Mengerjakan

Untuk memisahkan data menjadi train dan test, cukup jalankan cell berikut.

```{r}
# JANGAN UBAH CELL INI
# set seed for repeatability
set.seed(100)

# create split object 
train_test_split <- data %>% initial_split(prop = .8, strata = "Species")

train_tbl <- train_test_split %>% training()

test_tbl <- train_test_split %>% testing()

x_train = train_tbl %>% 
  select(Sepal.Length, Sepal.Width, Petal.Length, Petal.Width, Branch.Length, Branch.Width)

y_train = train_tbl %>%
  select(Species)

x_test = test_tbl %>%
  select(Sepal.Length, Sepal.Width, Petal.Length, Petal.Width, Branch.Length, Branch.Width)

y_test = test_tbl %>%
  select(Species)

```

##### KNN:

```{r}
### Silakan gunakan cell di bawah ini untuk mengerjakan KNN
set.seed(1)
y_train
```

```{r}
### Apabila tidak cukup, boleh menambahkan cell cell baru di bawah
y_train[,1,drop=TRUE]
```

```{r}

y_pred <- knn(train = x_train,
                        test = x_test,
                        cl = y_train[,1,drop=TRUE],
                        k = 3)

conf_mtrix <- confusionMatrix(data = y_pred, 
                              reference = as.factor(y_test[,1,drop=TRUE]))

conf_mtrix
```

```{r}

k_values = NULL
accuracy_pred = NULL

for(i in 1:10){
  set.seed(1)
  y_pred <- knn(train = x_train,
                          test = x_test,
                          cl = y_train[,1,drop=TRUE],
                          k = i)
  
  conf_mtrix <- confusionMatrix(data = y_pred, 
                                reference = as.factor(y_test[,1,drop=TRUE]))
  
  k_values <- c(k_values, i)
  accuracy_pred <- c(accuracy_pred, conf_mtrix$overall[1])
  
}
```

```{r}
library(ggplot2)
accuracy_data = as.data.frame(cbind(k_values, accuracy_pred))

ggplot(data = accuracy_data, aes(x = k_values, y = accuracy_pred))+
  geom_line()+
  geom_point()+
  scale_x_continuous(breaks = seq(1,20,by=1))
```

##### Logistic Regression:

```{r}
### Silakan gunakan cell di bawah ini untuk mengerjakan Logistic Regression
set.seed(1)
model_lgr<-glm(Species~.,
               data = train_tbl,
               family = "binomial")
# family = " binomial" means it contains only two outcomes.

summary(model_lgr)
```

```{r}
### Apabila tidak cukup, boleh menambahkan cell cell baru di bawah
y_pred <- predict(model_lgr, x_test, type = 'response')
y_pred <- ifelse(y_pred < 0.5, 0, 1)

conf_mtrix <- confusionMatrix(data = as.factor(y_pred), 
                                reference = as.factor(y_test[,1,drop=TRUE]))

conf_mtrix
```

```{r}
x_train_logit <- x_train %>%
  select(Petal.Length)

x_test_logit <- x_test %>% 
  select(Petal.Length)
```

```{r}
Species = train_tbl$Species
new_data_train = cbind(x_train_logit, Species)

model_lgr_2 <- glm(Species~.,
               data = new_data_train,
               family = "binomial")
# family = " binomial" means it contains only two outcomes.

summary(model_lgr_2)

```

```{r}
y_pred <- predict(model_lgr_2, x_test_logit, type = 'response')
y_pred <- ifelse(y_pred < 0.5, 0, 1)
conf_mtrix <- confusionMatrix(data = as.factor(y_pred), 
                                reference = as.factor(y_test[,1,drop=TRUE]))

conf_mtrix
```

### Tempat Menjawab

KNN:

-   Nilai k terbaik adalah 3

-   Akurasi tertinggi adalah 0.95

Logistic Regression:

-   Kolom dengan p-value di bawah 0.05 adalah Petal Length

-   Nilai akurasi tertinggi apabila melakukan logistic regression dengan kolom-kolom yang p-value nya \< 0.05 adalah 0.85
