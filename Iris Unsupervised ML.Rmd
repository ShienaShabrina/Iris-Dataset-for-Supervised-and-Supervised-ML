
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Supervised ML in R

Make sure to load the following libraries.

```{r}
library(tidyverse)
library(rsample)
#install.packages("class")
library(class)
#install.packages('gmodels')
library(gmodels)
#install.packages('caret')
library(caret)
#install.packages('rpart')
library(rpart)
#install.packages('rpart.plot')
library(rpart.plot)
```

## read file `iris_homework`

```{r}
data = readRDS('iris_homework')
```

```{r}
# set seed for repeatability
set.seed(100)

# create split object 
train_test_split <- data %>% initial_split(prop = .8, strata = "Species")

train_tbl <- train_test_split %>% training()

test_tbl <- train_test_split %>% testing()

x_train = train_tbl %>% 
  select(Sepal.Length, Sepal.Width, Petal.Length, Petal.Width, Branch.Length, Branch.Width)

y_train = train_tbl %>%
  select(Species)

x_test = test_tbl %>%
  select(Sepal.Length, Sepal.Width, Petal.Length, Petal.Width, Branch.Length, Branch.Width)

y_test = test_tbl %>%
  select(Species)

```

##### KNN:

```{r}
set.seed(1)
y_train
```

```{r}
y_train[,1,drop=TRUE]
```

```{r}

y_pred <- knn(train = x_train,
                        test = x_test,
                        cl = y_train[,1,drop=TRUE],
                        k = 3)

conf_mtrix <- confusionMatrix(data = y_pred, 
                              reference = as.factor(y_test[,1,drop=TRUE]))

conf_mtrix
```

```{r}

k_values = NULL
accuracy_pred = NULL

for(i in 1:10){
  set.seed(1)
  y_pred <- knn(train = x_train,
                          test = x_test,
                          cl = y_train[,1,drop=TRUE],
                          k = i)
  
  conf_mtrix <- confusionMatrix(data = y_pred, 
                                reference = as.factor(y_test[,1,drop=TRUE]))
  
  k_values <- c(k_values, i)
  accuracy_pred <- c(accuracy_pred, conf_mtrix$overall[1])
  
}
```

```{r}
library(ggplot2)
accuracy_data = as.data.frame(cbind(k_values, accuracy_pred))

ggplot(data = accuracy_data, aes(x = k_values, y = accuracy_pred))+
  geom_line()+
  geom_point()+
  scale_x_continuous(breaks = seq(1,20,by=1))
```

##### Logistic Regression:

```{r}
set.seed(1)
model_lgr<-glm(Species~.,
               data = train_tbl,
               family = "binomial")
# family = " binomial" means it contains only two outcomes.

summary(model_lgr)
```

```{r}
y_pred <- predict(model_lgr, x_test, type = 'response')
y_pred <- ifelse(y_pred < 0.5, 0, 1)

conf_mtrix <- confusionMatrix(data = as.factor(y_pred), 
                                reference = as.factor(y_test[,1,drop=TRUE]))

conf_mtrix
```

```{r}
x_train_logit <- x_train %>%
  select(Petal.Length)

x_test_logit <- x_test %>% 
  select(Petal.Length)
```

```{r}
Species = train_tbl$Species
new_data_train = cbind(x_train_logit, Species)

model_lgr_2 <- glm(Species~.,
               data = new_data_train,
               family = "binomial")
# family = " binomial" means it contains only two outcomes.

summary(model_lgr_2)

```

```{r}
y_pred <- predict(model_lgr_2, x_test_logit, type = 'response')
y_pred <- ifelse(y_pred < 0.5, 0, 1)
conf_mtrix <- confusionMatrix(data = as.factor(y_pred), 
                                reference = as.factor(y_test[,1,drop=TRUE]))

conf_mtrix
```

KNN:

-   Best K score is 3 

-   Highest accuracy is 0.95

Logistic Regression:

-   column with p-value under 0.05 is Petal Length

-   When doing logistic regression with columns with a p-value less than 0.05, the highest accuracy value is 0.85.
